# COâ‚‚ Emissions Data Engineering Project

This repository contains an ETL pipeline for the **processing and analysis of historical COâ‚‚ emissions data**.  
The project demonstrates a complete workflow â€” from data extraction to database integration and exploratory data analysis (EDA).  
Developed as part of the **Data Engineering** course at **ITMO University (2025)**.  
Written in **Python 3.13**.

---

## Project Overview

The main objectives of this project:
- Extraction of historical COâ‚‚ data from Google Drive  
- Preprocessing, cleaning, and type transformation  
- Storage of processed data in Parquet and PostgreSQL  
- Exploratory Data Analysis (EDA) for identifying global emission trends  

The project illustrates fundamental **data engineering** concepts and best practices for building reproducible ETL pipelines.

---

## Dataset

The dataset used in this project is available here:  
[ğŸ“ COâ‚‚ dataset (Google Drive)](https://drive.google.com/file/d/14wKDsdZ1HnI1-zcAPB59HnHJq6Th2z3X/view?usp=sharing)

**Source:** [Kaggle â€“ COâ‚‚ Emissions Across Countries, Regions, and Sectors](https://www.kaggle.com/datasets/shreyanshdangi/co-emissions-across-countries-regions-and-sectors/data)  
**Period covered:** 1850â€“2023  
**Contents:** Indicators on COâ‚‚, CHâ‚„, Nâ‚‚O emissions, energy use, GDP, and population across countries and sectors.

The dataset is automatically downloaded and processed by the ETL pipeline.

---

## Project Structure

```
MY-PROJECT/
â”œâ”€â”€ etl/                      # ETL pipeline package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py           # Data loading from Google Drive
â”‚   â”œâ”€â”€ transform.py         # Data transformation and cleaning
â”‚   â”œâ”€â”€ load.py              # Loading to DB and export to parquet
â”‚   â”œâ”€â”€ validate.py          # Data validation
â”‚   â””â”€â”€ main.py              # CLI entry point
â”‚â”€â”€ src
â”‚   â”‚
â”œâ”€â”€ api_example/              # API examples and integration
â”‚   â”œâ”€â”€ api_reader.py
â”‚   â”œâ”€â”€ environment.yml
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ notebook/                 # Jupyter notebooks
â”‚   â””â”€â”€ EDA.ipynb            # Exploratory Data Analysis
â”œâ”€â”€ parse_example/            # Data parsing examples
â”‚   â”œâ”€â”€ data_parser.py
â”‚   â”œâ”€â”€ environment.yml
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml           # Python project configuration
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt         # Python dependencies
```

---

## Installation and Setup

Follow these steps to set up the environment and run the project:

### 1. Create and activate a Conda environment
```bash
conda create -n my_env python=3.13 -y
conda activate my_env
```
### 2. Install dependencies 
```bash
pip install pandas pyarrow sqlalchemy psycopg2-binary python-dotenv
```
### 3. Configure database credentials

Create a `.env` file in the root of the project

### 4. ETL Pipeline Usage

The ETL process can be executed directly via command line.
 
 **Run the Full Pipeline**
You can execute the complete ETL process using the command below:

```bash
python etl/main.py --file_id <GoogleDrive_file_ID> --table <table_name>
```
```bash
--file_id â€” Google Drive file ID of the dataset
--table â€” PostgreSQL table name for storing the processed data
```

**Running Individual Stages**

You can also execute each stage of the ETL process separately:

# Extract stage â€“ download raw data from Google Drive
python -m etl.main extract --file_id 14wKDsdZ1HnI1-zcAPB59HnHJq6Th2z3X

# Transform stage â€“ clean and typecast raw data
python -m etl.main transform --input data/raw/raw_data.csv

# Load stage â€“ upload cleaned data into PostgreSQL
python -m etl.main load --input data/processed/clean_data.parquet --table khabibullina

### 5. **ETL Functionality**

#### **Extract**
-  Downloads the CSV dataset directly from **Google Drive**  
- Saves it locally in  
  `data/raw/raw_data.csv`

#### **Transform**
-  Converts the **year** column to `datetime` format  
-  Casts **population** to a nullable integer (`Int64`)  
-  Converts text columns â€” `iso_code`, `Name`, `Description` â€” to **categorical** types  
-  Saves the cleaned and transformed data as Parquet â†’  
  `data/processed/clean_data.parquet`

#### **Load**
-  Uploads up to **100 rows** into the PostgreSQL table (adds primary key `id`)  
-  Validates schema and data structure before insertion  
-  Prints confirmation message upon successful completion

**Example Run Output**
```bash
(my_env) MacBook-Air-Lejla:my_project lejla$ python -m etl.main --file_id 14wKDsdZ1HnI1-zcAPB59HnHJq6Th2z3X --table khabibullina
```
[RUN] Start ETL
[EXTRACT] Ğ¡Ñ‹Ñ€Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ² data/raw/raw_data.csv
[TRANSFORM] Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ² data/processed/clean_data.parquet
[LOAD] Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ 100 ÑÑ‚Ñ€Ğ¾Ğº Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ khabibullina
[DONE] ETL Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½


